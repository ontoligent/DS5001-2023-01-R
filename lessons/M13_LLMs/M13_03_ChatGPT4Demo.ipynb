{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6275c0-c10d-4ac2-bdb7-4ff58c56d94b",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "Course:    DS 5001\n",
    "Module:    13 LLMs\n",
    "Topic:     Demonstrate ChatGPT Plus with GPT-4\n",
    "Author:    R.C. Alvarado\n",
    "Date:      27 April 2023\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daffee5-cfc5-47a3-91a4-a93a040cd366",
   "metadata": {},
   "source": [
    "# Prompt 1\n",
    "\n",
    "Write a Python class that will scrape an arbitrary text from Project Gutenberg and convert the text into a Pandas dataframe of tokens with a multi-index that maps onto the text's structure, i.e. it's chapter, paragraph, sentence, and token numbers. Use NLTK to parse sentences and tokens, and get part-of-speech for each token. Also, use regular expressions to detect chapter breaks. Demonstrate with Jane Austen's Persuasion.\n",
    "\n",
    "> Here's a Python class that scrapes an arbitrary text from Project Gutenberg, converts the text into a Pandas DataFrame of tokens with a multi-index that maps onto the text's structure (chapter, paragraph, sentence, and token numbers), and also provides part-of-speech for each token. We will use the nltk library to parse sentences and tokens and get the part-of-speech tags, and use regular expressions to detect chapter breaks.\n",
    "\n",
    "> First, make sure you have the required libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b17eafe-dc50-4eb2-b56e-508eba3d80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: nltk in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: requests in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: joblib in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rca2t1/anaconda3/envs/eta/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee9712-0a47-439b-b248-ea6e3039270c",
   "metadata": {},
   "source": [
    "> Then, you can use the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7edaf5a-d082-46c4-bb9f-daf454fbd720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rca2t1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rca2t1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             token  pos\n",
      "0 1 0 0        The   DT\n",
      "      1    Project  NNP\n",
      "      2  Gutenberg  NNP\n",
      "      3      eBook   NN\n",
      "      4         of   IN\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "class GutenbergScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.text = self.get_text()\n",
    "        self.chapters = self.split_chapters()\n",
    "        self.data = self.parse_text()\n",
    "\n",
    "    def get_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "\n",
    "    def split_chapters(self):\n",
    "        return re.split(r\"\\bChapter\\b\", self.text, flags=re.IGNORECASE)\n",
    "\n",
    "    def parse_text(self):\n",
    "        parsed_data = []\n",
    "        for chap_no, chapter in enumerate(self.chapters):\n",
    "            paragraphs = re.split(r\"\\n\\n+\", chapter)\n",
    "            for para_no, paragraph in enumerate(paragraphs):\n",
    "                sentences = sent_tokenize(paragraph)\n",
    "                for sent_no, sentence in enumerate(sentences):\n",
    "                    tokens = word_tokenize(sentence)\n",
    "                    pos_tags = nltk.pos_tag(tokens)\n",
    "                    for token_no, (token, pos) in enumerate(pos_tags):\n",
    "                        parsed_data.append(\n",
    "                            (\n",
    "                                (chap_no, para_no, sent_no, token_no),\n",
    "                                {\"token\": token, \"pos\": pos},\n",
    "                            )\n",
    "                        )\n",
    "        return pd.DataFrame.from_dict(\n",
    "            dict(parsed_data), orient=\"index\", columns=[\"token\", \"pos\"]\n",
    "        )\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.gutenberg.org/files/105/105-h/105-h.htm\"\n",
    "    scraper = GutenbergScraper(url)\n",
    "    df = scraper.get_dataframe()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badfe0b-b1e2-4b15-a6fa-1d8dc919716f",
   "metadata": {},
   "source": [
    "> Replace the url variable with the URL of the specific Project Gutenberg page you want to scrape. In this case, we use Jane Austen's Persuasion HTML file. Running this script will print the first few rows of the parsed Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb82932-ef5d-40c9-a5a1-44988d093179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Project</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eBook</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">48</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">58</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>38</th>\n",
       "      <td>hear</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>about</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>new</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>eBooks</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101722 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                token  pos\n",
       "0  1  0 0         The   DT\n",
       "        1     Project  NNP\n",
       "        2   Gutenberg  NNP\n",
       "        3       eBook   NN\n",
       "        4          of   IN\n",
       "...               ...  ...\n",
       "48 58 0 38       hear   VB\n",
       "        39      about   IN\n",
       "        40        new   JJ\n",
       "        41     eBooks  NNS\n",
       "        42          .    .\n",
       "\n",
       "[101722 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923407e8-6b05-40d5-9a7a-9a139f839c56",
   "metadata": {},
   "source": [
    "# Prompt 2\n",
    "\n",
    "Rewrite this so that it only takes the Project Gutenburg ID in the initializer, and then figures out the regular expressions for chapter breaks in each case. This is necessary because sometimes chapters are indicated without the word \"Chapter\", and also we want to include things like letters as chapters.\n",
    "\n",
    "> To make the class more flexible, we can modify the __init__ method to take a Project Gutenberg ID instead of the URL, and then create a method that automatically generates the URL based on the ID. We can also update the split_chapters method to handle different chapter break patterns, including cases where the word \"Chapter\" is not used, and include things like letters as chapters.\n",
    "\n",
    "> Here's the updated class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42104689-91e8-4beb-bd9f-c5f620054d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rca2t1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/rca2t1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             token  pos\n",
      "0 1 0 0        The   DT\n",
      "      1    Project  NNP\n",
      "      2  Gutenberg  NNP\n",
      "      3      eBook   NN\n",
      "      4         of   IN\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "class GutenbergScraper:\n",
    "    def __init__(self, book_id):\n",
    "        self.book_id = book_id\n",
    "        self.url = self.get_url()\n",
    "        self.text = self.get_text()\n",
    "        self.chapters = self.split_chapters()\n",
    "        self.data = self.parse_text()\n",
    "\n",
    "    def get_url(self):\n",
    "        return f\"https://www.gutenberg.org/files/{self.book_id}/{self.book_id}-h/{self.book_id}-h.htm\"\n",
    "\n",
    "    def get_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "\n",
    "    def split_chapters(self):\n",
    "        patterns = [\n",
    "            r\"\\bChapter\\b\",\n",
    "            r\"\\bLetter\\b\",\n",
    "            r\"\\bSection\\b\",\n",
    "            r\"\\bPart\\b\",\n",
    "            r\"\\bBook\\b\",\n",
    "        ]\n",
    "        combined_pattern = r\"|\".join(patterns)\n",
    "        return re.split(combined_pattern, self.text, flags=re.IGNORECASE)\n",
    "\n",
    "    def parse_text(self):\n",
    "        parsed_data = []\n",
    "        for chap_no, chapter in enumerate(self.chapters):\n",
    "            paragraphs = re.split(r\"\\n\\n+\", chapter)\n",
    "            for para_no, paragraph in enumerate(paragraphs):\n",
    "                sentences = sent_tokenize(paragraph)\n",
    "                for sent_no, sentence in enumerate(sentences):\n",
    "                    tokens = word_tokenize(sentence)\n",
    "                    pos_tags = nltk.pos_tag(tokens)\n",
    "                    for token_no, (token, pos) in enumerate(pos_tags):\n",
    "                        parsed_data.append(\n",
    "                            (\n",
    "                                (chap_no, para_no, sent_no, token_no),\n",
    "                                {\"token\": token, \"pos\": pos},\n",
    "                            )\n",
    "                        )\n",
    "        return pd.DataFrame.from_dict(\n",
    "            dict(parsed_data), orient=\"index\", columns=[\"token\", \"pos\"]\n",
    "        )\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    book_id = 105  # Jane Austen's Persuasion\n",
    "    scraper = GutenbergScraper(book_id)\n",
    "    df = scraper.get_dataframe()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5b6e5-5691-428b-9416-25f528b14fed",
   "metadata": {},
   "source": [
    "> You can now provide the Project Gutenberg ID for the book you want to scrape, and the class will automatically generate the URL and handle different chapter break patterns. In the example above, we use the ID 105 for Jane Austen's Persuasion. Running the script will print the first few rows of the parsed Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488d941-1935-4160-a9ff-de50d58a9f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eta]",
   "language": "python",
   "name": "conda-env-eta-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
